{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f1dd62f",
   "metadata": {},
   "source": [
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/master/docs/release/tutorials/object-detection-in-videos.ipynb)&nbsp;&nbsp;\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/master/docs/release/tutorials/object-detection-in-videos.ipynb)\n",
    "\n",
    "# Object Detection in Videos\n",
    "\n",
    "In this tutorial, we'll demonstrate how to use Pixeltable to do frame-by-frame object detection, made simple through Pixeltable's video-related functionality:\n",
    "* automatic frame extraction\n",
    "* running complex functions against frames (in this case, the YOLOX object detection models)\n",
    "* reassembling frames back into videos\n",
    "We'll be working with a single video file from Pixeltable's test data repository.\n",
    "\n",
    "This tutorial assumes you've worked through the [Pixeltable Basics](https://pixeltable.github.io/pixeltable/tutorials/pixeltable-basics/) tutorial; if you haven't, it's probably a good idea to do so now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29054d06",
   "metadata": {},
   "source": [
    "## Creating a tutorial directory and table\n",
    "\n",
    "First, let's make sure the packages we need for this tutorial are installed: Pixeltable itself, and the YOLOX object detection library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "213d2677-e5c9-438b-bc11-e44f67433e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:04:25.342937Z",
     "iopub.status.busy": "2024-05-23T02:04:25.342363Z",
     "iopub.status.idle": "2024-05-23T02:04:26.452323Z",
     "shell.execute_reply": "2024-05-23T02:04:26.451882Z",
     "shell.execute_reply.started": "2024-05-23T02:04:25.342890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pixeltable git+https://github.com/Megvii-BaseDetection/YOLOX@ac58e0a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fff3df-021a-4d83-8c06-250225e0073f",
   "metadata": {},
   "source": [
    "As we saw in the Pixeltable Basics tutorial, all data in Pixeltable is stored in tables, which in turn reside in directories. We'll begin by creating a `video_tutorial` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da120aff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-23T02:04:26.454104Z",
     "iopub.status.busy": "2024-05-23T02:04:26.453916Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pixeltable database at: postgresql://postgres:@/pixeltable?host=/Users/orm/.pixeltable/pgdata\n"
     ]
    }
   ],
   "source": [
    "import pixeltable as pxt\n",
    "\n",
    "pxt.create_dir('video_tutorial', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970cf727",
   "metadata": {},
   "source": [
    "We create a table for our videos, with a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feacf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the tables if they exist, in case they were created by a previous\n",
    "# run of the tutorial\n",
    "pxt.drop_table('video_tutorial.frames', ignore_errors=True)\n",
    "pxt.drop_table('video_tutorial.videos', ignore_errors=True)\n",
    "\n",
    "# Create the `video_tutorial.videos` table\n",
    "videos_table = pxt.create_table('video_tutorial.videos', {'video': pxt.VideoType()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a079143",
   "metadata": {},
   "source": [
    "In order to interact with the frames, we take advantage of Pixeltable's component view concept: we create a \"view\" of our video table that contains one row for each frame of each video in the table. Pixeltable provides the built-in `FrameIterator` class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.iterators import FrameIterator\n",
    "\n",
    "frames_view = pxt.create_view(\n",
    "    'video_tutorial.frames',\n",
    "    videos_table,\n",
    "    # `fps` determines the frame rate; a value of `0`\n",
    "    # indicates the native frame rate of the video.\n",
    "    iterator=FrameIterator.create(video=videos_table.video, fps=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f52d627",
   "metadata": {},
   "source": [
    "You'll see that neither the `videos` table nor the `frames` view has any actual data yet, because we haven't yet added any videos to the table. However, the `frames` view is now configured to automatically track the `videos` table as new data shows up.\n",
    "\n",
    "The new view is automatically configured with six columns:\n",
    "- `pos` - a system column that is part of every component view\n",
    "- `video` - the column inherited from our base table (all base table columns are visible in any of its views)\n",
    "- `frame_idx`, `pos_msec`, `pos_frame`, `frame` - these four columns are created by the `FrameIterator` class.\n",
    "\n",
    "Let's have a look at the new view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c5d16",
   "metadata": {},
   "source": [
    "We'll now insert a single row into the videos table, containing a video of a busy intersection in Bangkok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a12da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_table.insert([{'video': 'https://raw.github.com/pixeltable/pixeltable/master/tests/data/videos/bangkok_half_res.mp4'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0de07",
   "metadata": {},
   "source": [
    "Notice that both the `videos` table and `frames` view were automatically updated, expanding the single video into 462 rows in the view. Let's have a look at `videos` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae9b15-40b0-45c9-af76-42e3f6c18b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d1110-4480-4c96-b36c-14a87a5e9c45",
   "metadata": {},
   "source": [
    "Now let's peek at the first five rows of `frames`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448c3da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames_view.select(frames_view.pos, frames_view.frame, frames_view.frame.width, frames_view.frame.height).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3efad",
   "metadata": {},
   "source": [
    "One advantage of using Pixeltable's component view mechanism is that Pixeltable does not physically store the frames. Instead, Pixeltable re-extracts the frames on retrieval using the frame index, which can be done very efficiently and avoids any storage overhead (which can be quite substantial for video frames)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755edf69",
   "metadata": {},
   "source": [
    "## Object Detection with Pixeltable\n",
    "\n",
    "Now let's apply an object detection model to our frames. Pixeltable includes built-in support for a number of models; we're going to use the YOLOX family of models, which are lightweight models with solid performance. We first import the `yolox` Pixeltable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2f4d8-1725-40bf-8614-9420573bb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.ext.functions.yolox import yolox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47371709-2932-4481-ab5b-5e0596027dbe",
   "metadata": {},
   "source": [
    "Pixeltable functions operate on columns and expressions using standard Python function call syntax. Here's an example that shows how we might experiment with applying one of the YOLOX models to the first few frames in our video, using Pixeltable's powerful `select` comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6465d-ff30-4449-aa75-f443c2879682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the results of applying the `yolox_tiny` model to the first few frames in the table.\n",
    "\n",
    "frames_view.select(frames_view.frame, yolox(frames_view.frame, model_id='yolox_tiny')).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5105197-3c9a-48b3-a730-73ea752a78e7",
   "metadata": {},
   "source": [
    "It may appear that we just ran the YOLOX inference over the entire view of 462 frames, but remember that Pixeltable evaluates expressions lazily: in this case, it only ran inference over the 3 frames that we actually displayed.\n",
    "\n",
    "The inference output looks like what we'd expect, so let's add a _computed column_ that runs inference over the entire view (we first encountered computed columns in the Pixeltable Basics tutorial). Remember that once a computed column is created, Pixeltable will update it incrementally any time new rows are added to the view. This is a convenient way to incorporate inference (and other operations) into data workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a computed column to compute detections using the `yolox_tiny` model.\n",
    "# We'll adjust the confidence threshold down a bit (the default is 0.5) to pick up even more\n",
    "# bounding boxes.\n",
    "\n",
    "frames_view['detect_yolox_tiny'] = yolox(frames_view.frame, model_id='yolox_tiny', threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3507a-ae94-4967-9805-2477a0d37ec4",
   "metadata": {},
   "source": [
    "The new column is now part of the schema of the `frames` view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b55daa-7265-4210-831f-dc486ece5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811d2795-d00e-443c-9d32-9ff56881ffde",
   "metadata": {},
   "source": [
    "The data in the computed column is now stored for fast retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca127b-2961-4015-ab0d-dacf68e3c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view.select(frames_view.frame, frames_view.detect_yolox_tiny).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b484dcda",
   "metadata": {},
   "source": [
    "Now let's create a new set of images, in which we superimpose the detected bounding boxes on top of the original images. There's no built-in Pixeltable function to do this, but we can easily create our own. We'll use the `@pxt.udf` decorator for this, as we first saw in the Pixeltable Basics tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import PIL.ImageDraw\n",
    "\n",
    "@pxt.udf\n",
    "def draw_boxes(img: PIL.Image.Image, boxes: list[list[float]]) -> PIL.Image.Image:\n",
    "    result = img.copy()  # Create a copy of `img`\n",
    "    d = PIL.ImageDraw.Draw(result)\n",
    "    for box in boxes:\n",
    "        d.rectangle(box, width=3)  # Draw bounding box rectangles on the copied image\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816012b9-3069-4ded-942e-00e18d6a10b1",
   "metadata": {},
   "source": [
    "This function takes two arguments, `img` and `boxes`, and returns the new, annotated image. We could create a new computed column to hold the annotated images, but we don't have to; sometimes it's easier just to use a `select` comprehension, as we did when we were first experimenting with the detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051d8c9c-1599-4527-a910-b0a9aa76fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view.select(frames_view.frame, draw_boxes(frames_view.frame, frames_view.detect_yolox_tiny.bboxes)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f07d2e",
   "metadata": {},
   "source": [
    "Our `select` comprehension ranged over the entire table, but just as before, Pixeltable computes the output lazily: image operations are performed at retrieval time, so in this case, Pixeltable drew the annotations just for the one frame that we actually displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6943594",
   "metadata": {},
   "source": [
    "Looking at individual frames gives us some idea of how well our detection algorithm works, but it would be more instructive to turn the visualization output back into a video.\n",
    "\n",
    "We do that with the built-in function `make_video()`, which is an aggregation function that takes a frame index (actually: any expression that can be used to order the frames; a timestamp would also work) and an image, and then assembles the sequence of images into a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce224393",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view.select(pxt.functions.make_video(\n",
    "    frames_view.pos, draw_boxes(frames_view.frame, frames_view.detect_yolox_tiny.bboxes)\n",
    ")).group_by(videos_table).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43742d76-5c40-4aac-821f-01fbbb4fdc80",
   "metadata": {},
   "source": [
    "## Comparing Object Detection Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79351f4-2406-47dc-88a4-60454ed64ea3",
   "metadata": {},
   "source": [
    "Now suppose we want to experiment with a more powerful object detection model, to see if there is any improvement in detection quality. We can create an additional column to hold the new inferences. The larger model takes longer to download and run, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb881e1-177a-4df0-a8da-e59fdcf12aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the larger `yolox_m` (medium) model.\n",
    "\n",
    "frames_view['detect_yolox_m'] = yolox(frames_view.frame, model_id='yolox_m', threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760cc09d-b256-426f-9f27-d76e400500f9",
   "metadata": {},
   "source": [
    "Let's see the results of the two models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7068a943-2f7c-483e-89ed-b1cf04cd3641",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view.select(pxt.functions.make_video(\n",
    "    frames_view.pos, draw_boxes(frames_view.frame, frames_view.detect_yolox_tiny.bboxes)\n",
    "), pxt.functions.make_video(\n",
    "    frames_view.pos, draw_boxes(frames_view.frame, frames_view.detect_yolox_m.bboxes)\n",
    ")).group_by(videos_table).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ff749-d952-416d-8284-2b308deed662",
   "metadata": {},
   "source": [
    "Running the videos side-by-side, we can see that the larger model is higher in quality: less flickering, with more stable boxes from frame to frame.\n",
    "\n",
    "## Evaluating Models Against a Ground Truth\n",
    "\n",
    "In order to do a quantitative evaluation of model performance, we need a ground truth to compare them against. Let's generate some (synthetic) \"ground truth\" data by running against the largest YOLOX model available. It will take even longer to cache and evaluate this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd00742-b2f2-44ae-98ae-0144c8385bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view['detect_yolox_x'] = yolox(frames_view.frame, model_id='yolox_x', threshold=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fa70c-b728-447d-b658-c0bd16205a38",
   "metadata": {},
   "source": [
    "Let's have a look at our enlarged view, now with three `detect` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7750b7-9f01-40b2-8a0e-5147c225e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55cbb90-0338-4a50-9b70-25e6ff5eefdd",
   "metadata": {},
   "source": [
    "We're going to be evaluating the generated detections with the commonly-used [mean average precision](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/) metric (mAP).\n",
    "\n",
    "The mAP metric is based on per-frame metrics, such as true and false positives per detected class, which are then aggregated into a single (per-class) number. In Pixeltable, functionality is available via the `eval_detections()` and `mean_ap()` built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6ca7f-5ce7-4e77-b90f-ecc039922041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.eval import eval_detections, mean_ap\n",
    "\n",
    "frames_view['eval_yolox_tiny'] = eval_detections(\n",
    "    pred_bboxes=frames_view.detect_yolox_tiny.bboxes,\n",
    "    pred_labels=frames_view.detect_yolox_tiny.labels,\n",
    "    pred_scores=frames_view.detect_yolox_tiny.scores,\n",
    "    gt_bboxes=frames_view.detect_yolox_x.bboxes,\n",
    "    gt_labels=frames_view.detect_yolox_x.labels\n",
    ")\n",
    "\n",
    "frames_view['eval_yolox_m'] = eval_detections(\n",
    "    pred_bboxes=frames_view.detect_yolox_m.bboxes,\n",
    "    pred_labels=frames_view.detect_yolox_m.labels,\n",
    "    pred_scores=frames_view.detect_yolox_m.scores,\n",
    "    gt_bboxes=frames_view.detect_yolox_x.bboxes,\n",
    "    gt_labels=frames_view.detect_yolox_x.labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e65880-c59b-464b-baf1-4fa1a94a1de7",
   "metadata": {},
   "source": [
    "Let's take a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52ac5f-eb12-483d-bce2-8f2e6666a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view.select(frames_view.eval_yolox_tiny, frames_view.eval_yolox_m).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce04ee4-fa51-49cf-a71d-b96bfa4290ad",
   "metadata": {},
   "source": [
    "The computation of the mAP metric is now simply a query over the evaluation output, aggregated with the `mean_ap()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b852c-f980-46c2-b75d-f1c5d4529fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_view.select(mean_ap(frames_view.eval_yolox_tiny), mean_ap(frames_view.eval_yolox_m)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfd91b-15ae-4c68-8c7a-d1f17a66ec77",
   "metadata": {},
   "source": [
    "This two-step process allows you to compute mAP at every granularity: over your entire dataset, only for specific videos, only for videos that pass a certain filter, etc. Moreover, you can compute this metric any time, not just during training, and use it to guide your understanding of your dataset and how it affects the quality of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8852831-f2a5-4a13-b90b-b9d8015009c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixeltable_39",
   "language": "python",
   "name": "pixeltable_39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
